% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/RunModels.R
\name{RunModels}
\alias{RunBootstrap}
\alias{RunCV}
\alias{RunModels}
\title{RunModels}
\usage{
RunCV(form.list, data, classifier, cv = 2, para, opt.para = FALSE,
  val.propo = 0.1, return.model = FALSE, varimp = TRUE, parallel = TRUE,
  mc.cores = getOption("mc.cores", 2L), seed = 12345, ...)

RunBootstrap(form.list, data, classifier, nBoots = 5, para,
  opt.para = FALSE, val.propo = 0.1, return.model = FALSE,
  varimp = FALSE, parallel = TRUE, mc.cores = getOption("mc.cores", 2L),
  seed = 12345, ...)
}
\arguments{
\item{form.list}{named list of formulas}

\item{data}{data matrix}

\item{classifier}{character list of classification models. See output of
\code{names(TrainModels())}.}

\item{cv,nBoots}{number of cv folds, bootsraps}

\item{para}{named list of model hyper-parameters}

\item{opt.para}{(logical) perform parameter optimization ?}

\item{val.propo}{proportion of test set to set aside for seelection of classification threshold}

\item{return.model}{(logical) return trained model and predicted probabilities?}

\item{varimp}{(logical) return variable importance ?}

\item{parallel}{(logical) train and test models in parallel using parallel package ?}

\item{mc.cores}{number of cores to use in parallel mode}

\item{seed}{random seed for reproducibility}

\item{\dots}{further arguments passed to or from other methods.}
}
\value{
list of length cv or nBoots containing performance measures for each
classifier as a named list with
\item{model}{trained model}
\item{para}{named list of model hyper-paramters (tunned values if opt.para = TRUE)}
\item{run.time}{compute time}
\item{varimp}{variable importance if classifier is GLM, GBM or RF}
\item{perf}{a list with two data.frames: val.perf and tst.perf containing performance measures
for validation and test sets }
}
\description{
Run cross-validation or bootstrap in series or parallel


}
\details{
\enumerate{
  \item \code{RunCV}:
  This function runs in cross-validation style to train, test and select optimal
  model hyper-parameters. A grid search is performed for each cross-validation using the grid
  paramerter in \code{para} and the best parameters used to train the final model.
  \item \code{RunBootstrap}: Runs models in bootstrap (balanced) style to train, test and if desired also
  perform paramter optimization for each bootstrap just as in \code{RunCV}. \code{RunCV} can be run first and
  pass the output to \code{get.para} to select the para returned by the cross-validation with the
  best performance.
}
}
\examples{
\dontrun{
para = list(
method = "cv",
tuneLength = 4,
number = 3,
GLM = c(),
ELR = list(grid = expand.grid(gamma = c(0.01, 0.25), p =  c(100, 400)),
           ken = "sigmoid", p = 400, gamma = 10.05),
GBM = list(n.trees=10, interaction.depth=1, shrinkage=0.01,
           mstop = 100, n.minobsinnode = 10,prune = "no",
           grid = expand.grid(interaction.depth = c(1, 3, 5),
                   n.trees= c(10, 20, 50), shrinkage=c(0.01),
                   n.minobsinnode=10)),
RF = list(ntree = 100, mtry = 5),
pLDA = list(lambda = 0.5),
avNNET = list(nnet.size = 50, nnet.decay= 0.001, nnet.bag = 10, maxit = 200),
SVM = list(C = .05, sigma = 0.5, kernel= "rbfdot", grid = expand.grid(C = c(0.01, 1, 10, 100),
                                 sigma = c(0.001, 0.05, 0.5))),
GLMnet = list(alpha = 1, lambda = 1, grid = expand.grid(alpha=c(0,0.5, 1),
             lambda= c(0.01, 0.05, 0.1, 0.5)))
)
# Pima Indian diabetes data
link <-"http://archive.ics.uci.edu/ml/machine-learning-databases/
pima-indians-diabetes/pima-indians-diabetes.data"
dat <- read.table(file = link, header = FALSE, sep = ",")
rhs.vars <- c("n.preg","glu","bp","skin","insulin","bmi","ped","age") ## independent variables
outcome <- c("class1", "class2") ## outcome: diabetic or not
dat <- cbind(dat, dat[, 9])
names(dat) <- c(rhs.vars, outcome)
dat[, rhs.vars] <-  normalize01(dat[, rhs.vars])

form <- lapply(outcome, function(y) as.formula(paste0(paste0(y, "~"),
paste0(rhs.vars, collapse= "+"))))
names(form) <- outcome
classifier <- c("GLM", "GBM" , "ELR", "avNNET", "GLMnet", "SVM", "RF")

cv.res <- RunCV(form=form, data=dat, classifier=classifier, cv = 5,
                para=para, opt.para=TRUE, val.propo = 0.10, return.model = TRUE,
               varimp = TRUE, parallel = FALSE, mc.cores = cores, seed=12345)
cv.para <- get.para(object=cv.res, para=para)
cv.perf <-  get.cvResults(object=cv.res)

boot.res <- RunBootstrap(form=form, data=dat, classifier=classifier,
                  nBoots = 5, para=cv.para, opt.para=FALSE, val.propo = 0.10,
                  return.model = FALSE, varimp = FALSE, parallel = FALSE,
                    mc.cores = cores, seed=12345)
boot.perf <-  get.bootResults(object=boot.res)

}
}
\author{
Che Ngufor <Ngufor.Che@mayo.edu>
}

